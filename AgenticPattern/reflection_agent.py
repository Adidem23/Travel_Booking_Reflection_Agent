import asyncio
import os
from agents import Agent, Runner
from agents.mcp import MCPServerStdio
from mcp import ClientSession, StdioServerParameters, stdio_client
from utils.completions import build_prompt_structure, FixedFirstChatHistory, update_chat_history
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_mcp_adapters.tools import load_mcp_tools
from langchain_core.messages import SystemMessage
from langgraph.prebuilt import create_react_agent

load_dotenv()

GOOGLE_GEMINI_API_KEY=os.getenv("GOOGLE_GEMINI_API_KEY")

BASE_GENERATION_SYSTEM_PROMPT = """
Your task is to Generate the best content possible for the user's request.
If the user provides critique, respond with a revised version of your previous attempt.
You must always output the revised content.
"""

BASE_REFLECTION_SYSTEM_PROMPT = """
You are tasked with generating critique and recommendations to the user's generated content.
If the user content has something wrong or something to be improved, output a list of recommendations
and critiques. If the user content is ok and there's nothing to change, output this: <OK>
"""

class ReflectionAgent:
    def __init__(self, name: str = "Travel Booking Agent",
                 instructions: str = "You are a travel booking agent which creates a best plan according to the user requirements"):
        self.name = name
        self.instructions = instructions


    ## OPEN AI 

    # async def _request_completion(self, history: list) -> str:
    #     async with MCPServerStdio(
    #         name=self.name,
    #         params={
    #             "command": "python",
    #             "args": ["controllers/server.py"],
    #         },
    #     ) as server:
    #         agent = Agent(
    #             name=self.name,
    #             instructions=self.instructions,
    #             mcp_servers=[server],
    #         )
    #         result = await Runner.run(agent, history)
    #         return result.final_output
        

   ## GOOGLE GEMINI FOR TESTING 

    async def _request_completion(self,history:list):

        TRAVEL_REFLECTION_SYSTEM_PROMPT = """
            You are a Travel Booking Reflection Agent.  
            Your role is to carefully evaluate and critique travel itineraries generated by another agent.  

            Guidelines:  
            - If the itinerary has errors, missing details, or could be improved, provide clear and actionable recommendations.  
            - Focus on important aspects such as travel feasibility, timing, budget alignment, activity variety, cultural relevance, and user preferences.  
            - Be concise and structured: list issues and suggestions for improvement.  
            - If the itinerary is already excellent and requires no changes, respond only with: <OK>  

            Your critique should always be constructive, helpful, and aimed at refining the travel plan so that it fully satisfies the userâ€™s request.
        """

        chat_model=ChatGoogleGenerativeAI(
                model="gemini-2.5-flash",
                api_key=GOOGLE_GEMINI_API_KEY
        )
        
        server_script_path="controllers/server.py" 
        command = "python" 
        server_params = StdioServerParameters( command=command, args=[server_script_path], env=None ) 
        async with stdio_client(server_params) as (read, write): 
                async with ClientSession(read, write) as session:
                    await session.initialize()
                    tools = await load_mcp_tools(session)
                    actual_System_Prompt=SystemMessage(TRAVEL_REFLECTION_SYSTEM_PROMPT)

                    agent=create_react_agent(chat_model,tools,prompt=actual_System_Prompt)
                    result = agent.invoke(history)
                    return result[-1]["content"]
                    
    async def generate(self, generation_history: list) -> str:
        return await self._request_completion(generation_history)

    async def reflect(self, reflection_history: list) -> str:
        return await self._request_completion(reflection_history)

    async def run(
        self,
        user_msg: str,
        generation_system_prompt: str = "",
        reflection_system_prompt: str = "",
        n_steps: int = 10,
    ) -> str:
        generation_system_prompt += BASE_GENERATION_SYSTEM_PROMPT
        reflection_system_prompt += BASE_REFLECTION_SYSTEM_PROMPT

        generation_history = FixedFirstChatHistory(
            [
                build_prompt_structure(prompt=generation_system_prompt, role="system"),
                build_prompt_structure(prompt=user_msg, role="user"),
            ],
            total_length=3,
        )

        reflection_history = FixedFirstChatHistory(
            [build_prompt_structure(prompt=reflection_system_prompt, role="system")],
            total_length=3,
        )

        for step in range(n_steps):
            generation = await self.generate(generation_history)
            update_chat_history(generation_history, generation, "assistant")
            update_chat_history(reflection_history, generation, "user")

            critique = await self.reflect(reflection_history)

            if "<OK>" in critique:
                print("\n\nStop Sequence found. Stopping the reflection loop ... \n\n")
                break

            update_chat_history(generation_history, critique, "user")
            update_chat_history(reflection_history, critique, "assistant")

        return generation